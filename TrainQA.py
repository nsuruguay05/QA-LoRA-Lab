import re
import torch
import torch.profiler as profiler
from LoadDataset import load_quales_train, load_quales_val
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainerCallback, set_seed
from trl import SFTConfig, SFTTrainer
from peft import LoraConfig, PeftModel
from tqdm import tqdm
import pandas as pd
import gc
import math

class ProfilerCallback(TrainerCallback):
    def __init__(self, profiler):
        self.profiler = profiler
    
    def on_step_end(self, args, state, control, **kwargs):
        self.profiler.step()

class TrainQA:
    def __init__(self, model_name, new_model_name, base_folder, train_dataset, val_dataset, qlora=False, seed=1234):
        """ Initializes the TrainQA class with model and dataset information.
        Args:
            model_name (str): The Hugging Face id of the pre-trained model.
            new_model_name (str): The name for the new fine-tuned model.
            base_folder (str): The base folder to save outputs and logs.
            train_dataset (str): The path to the training dataset.
            val_dataset (str): The path to the validation dataset.
            qlora (bool): Whether to use QLoRA quantization.
            seed (int): Random seed for reproducibility.
        """
        self.model_name = model_name
        self.new_model_name = new_model_name
        self.base_folder = base_folder
        self.profiler = None
        self.seed = seed
        set_seed(self.seed)

        # Dict with evaluation and profiler metrics
        self.metrics = {
            "exact_match": None,
            "f1_score": None,
            "peak_memory_gpu_mb": None,
            "avg_gpu_time_per_step_ms": None,
            "avg_cpu_time_per_step_ms": None,
            "total_training_time_s": None,
            "training_throughput_samples_per_s": None,
            "training_throughput_steps_per_s": None,
            "total_steps": None,
        }

        # Quantization Config
        if qlora:
            self.quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=False
            )
        else:
            self.quant_config = None

        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quant_config,
            device_map="auto",
            trust_remote_code=True,
        )
        self.model = self.base_model

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, padding_side='left')
        self.tokenizer.pad_token = self.tokenizer.eos_token

        self.train_dataset = load_quales_train(train_dataset, self.tokenizer)
        self.val_dataset = load_quales_val(val_dataset, self.tokenizer)
    
    def add_lora_adapter(self, path):
        """ Adds a LoRA adapter to the model from the specified path.
        Args:
            path (str): The path to the LoRA adapter.
        """
        self.model = PeftModel.from_pretrained(self.model, path)

    def generate(self, prompt, max_tok=250):
        """ Generates text based on the provided input text using greedy decoding.
        Args:
            prompt (str): The input text to generate from.
            max_tok (int): The maximum number of tokens to generate.
        """
        # Tokenize the input text
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True).to(self.model.device)
        
        # Greedy decoding
        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_tok,
                do_sample=False,
                top_p=None,
                temperature=None,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode the result
        generated_texts = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)
        return [generated_text.split("assistant")[1].strip() for generated_text in generated_texts]
    
    def exact_match(self, generated_text, answers):
        """ Computes the exact match score between generated text and answers.
        Args:
            generated_text (str): The text generated by the model.
            answers (list of str): The list of correct answers to compare against.
        Returns:
            float: The exact match score.
        """
        generated_text = "" if generated_text.lower() == "no responde" else generated_text
        return float(generated_text in answers)
    
    def f1_score(self, generated_text, answers):
        """ Computes the F1 score between generated text and answers.
        Args:
            generated_text (str): The text generated by the model.
            answers (list of str): The list of correct answers to compare against.
        Returns:
            float: The F1 score.
        """
        if generated_text.lower() == "no responde":
            return 1.0 if "" in answers else 0.0

        # Remove puntuation and convert to lowercase
        generated_text = re.sub(r'[^\w\s]', '', generated_text.lower())
        answers = [re.sub(r'[^\w\s]', '', answer.lower()) for answer in answers]

        # Generate bags of words
        generated_words = set(generated_text.split())
        answer_words = [set(answer.split()) for answer in answers]

        # Calculate precision and recall
        scores = []
        for answer in answer_words:
            intersection = generated_words.intersection(answer)
            if len(generated_words) == 0 or len(answer) == 0:
                precision = recall = 0.0
            else:
                precision = len(intersection) / len(generated_words)
                recall = len(intersection) / len(answer)
            if precision + recall == 0:
                f1 = 0.0
            else:
                f1 = 2 * (precision * recall) / (precision + recall)
            scores.append(f1)

        # Return the maximum F1 score across all answers
        return max(scores) if scores else 0.0
        
    
    def evaluate(self, batch_size=8, file_name="eval_results.csv"):
        """ Evaluates the model on the validation dataset using exact match and F1 score.
        Saves the results to the base folder. 
        
        Args:
            batch_size (int): The batch size for evaluation.
            file_name (str): The name of the CSV file to save results.
        """
        batch_size = 8
        exact_match_scores = []
        f1_scores = []
        results_df = pd.DataFrame(columns=['generated_text', 'exact_match', 'f1_score'])

        for i in tqdm(range(0, len(self.val_dataset), batch_size), desc="Evaluating"):
            # Get a batch of data
            batch = self.val_dataset[i:i + batch_size]
            prompts = [item for item in batch['prompt']]
            answers = [item for item in batch['answers']]

            # Generate text for the batch
            generated_texts = self.generate(prompts)

            # Calculate exact match and F1 scores
            for generated_text, answer in zip(generated_texts, answers):
                exact_match_scores.append(self.exact_match(generated_text, answer))
                f1_scores.append(self.f1_score(generated_text, answer))
            
            # Save results to a CSV file
            for generated_text, exact_match, f1 in zip(generated_texts, exact_match_scores[-batch_size:], f1_scores[-batch_size:]):
                new_row = pd.DataFrame({
                    'generated_text': [generated_text],
                    'exact_match': [exact_match],
                    'f1_score': [f1]
                })
                results_df = pd.concat([results_df, new_row], ignore_index=True)
            results_df.to_csv(f"{self.base_folder}{file_name}", index=False)

        # Calculate average scores
        avg_exact_match = sum(exact_match_scores) / len(exact_match_scores) if exact_match_scores else 0.0
        avg_f1_score = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0

        self.metrics["exact_match"] = avg_exact_match
        self.metrics["f1_score"] = avg_f1_score
        
        print("Evaluation Results:")
        print(f"Average Exact Match: {avg_exact_match:.4f}")
        print(f"Average F1 Score: {avg_f1_score:.4f}")
    
    def profiler_info(self):
        """ Updates the metrics dictionary with peak memory and average times."""
        if self.profiler is None:
            print("No profiler data available.")
            return
        
        events = self.profiler.profiler.function_events
    
        cpu_us  = sum(e.self_cpu_time_total  for e in events)
        cuda_us = sum(getattr(e, "self_cuda_time_total", 0) for e in events)

        num_active_steps = self.active * self.repeat
        
        avg_cpu_time_ms = (cpu_us / num_active_steps) / 1000
        avg_cuda_time_ms = (cuda_us / num_active_steps) / 1000

        peak_memory_mb = torch.cuda.max_memory_allocated() / (1024**2)
        
        # Update metrics dictionary
        self.metrics["peak_memory_gpu_mb"] = peak_memory_mb
        self.metrics["avg_gpu_time_per_step_ms"] = avg_cuda_time_ms
        self.metrics["avg_cpu_time_per_step_ms"] = avg_cpu_time_ms
        
        print("Profiler statistics:")
        print(f"  - Peak GPU Memory Usage: {self.metrics['peak_memory_gpu_mb']:.2f} MB")
        print(f"  - Avg. GPU Time per Step: {self.metrics['avg_gpu_time_per_step_ms']:.2f} ms")
        print(f"  - Avg. CPU Time per Step: {self.metrics['avg_cpu_time_per_step_ms']:.2f} ms")

    def train(self, lora_alpha=32, lora_dropout=0.1, r=16, epochs=3, batch_size=8, learning_rate=2e-4, eval_and_profile=True):
        """ Fine-tunes the model using LoRA and evaluates it after training.
        Args:
            lora_alpha (int): The LoRA alpha parameter.
            lora_dropout (float): The LoRA dropout rate.
            r (int): The LoRA rank.
            epochs (int): The number of training epochs.
            batch_size (int): The training batch size.
            learning_rate (float): The learning rate for training.
            eval_and_profile (bool): Whether to evaluate and profile after training.
        """
        # LoRA Config
        peft_parameters = LoraConfig(
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            r=r,
            bias="none",
            task_type="CAUSAL_LM"
        )

        # Training parameters
        train_params = SFTConfig(
            output_dir=f"{self.base_folder}outputs/{self.new_model_name}",
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=1,
            optim="paged_adamw_32bit",
            save_steps=500,
            logging_steps=25,
            learning_rate=learning_rate,
            fp16=False,
            bf16=False,
            group_by_length=True,
            lr_scheduler_type="linear",
            report_to="tensorboard",
            packing = False
        )

        # Trainer
        fine_tuning = SFTTrainer(
            model=self.model,
            train_dataset=self.train_dataset,
            peft_config=peft_parameters,
            args=train_params
        )

        # Calculate wait time for profiling based on the number of steps and batch size
        num_steps = len(self.train_dataset) // batch_size * epochs
        self.warmup = 1
        self.active = 3
        self.repeat = 4
        self.wait = max(0, (num_steps // self.repeat) - (self.warmup + self.active))

        # Training with profiling
        with profiler.profile(
            activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],
            record_shapes=True,
            profile_memory=True,
            schedule=profiler.schedule(wait=self.wait, warmup=self.warmup, active=self.active, repeat=self.repeat),
            on_trace_ready=profiler.tensorboard_trace_handler(f"{self.base_folder}profiler")
        ) as prof:
            fine_tuning.add_callback(ProfilerCallback(prof))
            train_output = fine_tuning.train()

        # Update metrics with training results
        self.metrics["total_training_time_s"] = train_output.metrics["train_runtime"]
        self.metrics["training_throughput_samples_per_s"] = train_output.metrics["train_samples_per_second"]
        self.metrics["training_throughput_steps_per_s"] = train_output.metrics["train_steps_per_second"]

        steps_per_epoch = math.ceil(len(self.train_dataset) / batch_size)
        self.metrics["total_steps"] = steps_per_epoch * epochs

        print(f"Total training time: {self.metrics['total_training_time_s']:.2f} seconds")
        print(f"Training throughput: {self.metrics['training_throughput_samples_per_s']:.2f} samples/sec")
        print(f"Training throughput: {self.metrics['training_throughput_steps_per_s']:.2f} steps/sec")

        self.profiler = prof
        self.model = fine_tuning.model

        if eval_and_profile:
            # Print profiler information
            self.profiler_info()
            # Evaluate the model after training
            self.evaluate(batch_size=batch_size, file_name="eval_results_dev.csv")
    
    def reset(self):
        """ 
        Resets the model to its original state before training, ready for a new run.
        This function unloads the PEFT adapter and cleans up GPU memory.
        """
        # Unload the LoRA adapter if the model is a PeftModel
        if isinstance(self.model, PeftModel):
            self.model = self.model.unload()
            print("PEFT adapter unloaded.")
        
        # The active model is now the original base model
        self.model = self.base_model

        # Clean up memory
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        print("GPU cache cleared.")

        # Reset metrics and profiler for the next run
        self.profiler = None
        self.metrics = {
            "exact_match": None,
            "f1_score": None,
            "peak_memory_gpu_mb": None,
            "avg_gpu_time_per_step_ms": None,
            "avg_cpu_time_per_step_ms": None,
            "total_training_time_s": None,
            "training_throughput_samples_per_s": None
        }
        print("Metrics and profiler have been reset.")
